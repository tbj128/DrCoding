{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformer Commands\n",
    "\n",
    "### TOY\n",
    "#### Train \n",
    "python run.py train --train-src=../data_toy/train.data --dev-src=../data_toy/test.data --vocab=vocab.toy.json --glove-path=../embeddings/glove.6B.100d.txt --target-length=256 --valid-niter=2 --model=reformer --transformer-depth=2 --transformer-heads=8 --log-every=1\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "python run.py train --train-src=../data_toy/train.data --dev-src=../data_toy/test.data --vocab=vocab.toy.json --glove-path=../embeddings/glove.6B.100d.txt --target-length=128 --valid-niter=10 --model=reformer --transformer-depth=1 --transformer-heads=2 --log-every=1 --lr=0.0003\n",
    "epoch 10, iter 38, avg. loss 0.00 total examples 32, speed 1097.83 words/sec, time elapsed 119.10 sec\n",
    "I0308 14:39:46.059379 140735498576768 run.py:373] epoch 10, iter 38, avg. loss 0.00 total examples 32, speed 1097.82 words/sec, time elapsed 119.10 sec\n",
    "Loss is 0.0012591113336384296\n",
    "epoch 10, iter 39, avg. loss 0.00 total examples 32, speed 970.54 words/sec, time elapsed 123.32 sec\n",
    "I0308 14:39:50.280092 140735498576768 run.py:373] epoch 10, iter 39, avg. loss 0.00 total examples 32, speed 970.53 words/sec, time elapsed 123.32 sec\n",
    "Loss is 0.00094817322678864\n",
    "epoch 10, iter 40, avg. loss 0.00 total examples 4, speed 900.87 words/sec, time elapsed 123.89 sec\n",
    "I0308 14:39:50.848818 140735498576768 run.py:373] epoch 10, iter 40, avg. loss 0.00 total examples 4, speed 900.79 words/sec, time elapsed 123.89 sec\n",
    "VALIDATION: 40 | Precision 1.0, recall 1.0, f1 1.0, accuracy: 1.0\n",
    "I0308 14:39:51.198707 140735498576768 run.py:389] VALIDATION: 40 | Precision 1.0, recall 1.0, f1 1.0, accuracy: 1.0\n",
    "\n",
    "---\n",
    "\n",
    "[WITH POSITION EMBEDDING]\n",
    "\n",
    "python run.py train --train-src=../data_toy/train.data --dev-src=../data_toy/test.data --vocab=vocab.toy.json --glove-path=../embeddings/glove.6B.100d.txt --target-length=128 --valid-niter=10 --model=reformer --transformer-depth=1 --transformer-heads=2 --log-every=1 --lr=0.0003\n",
    "\n",
    "epoch 10, iter 39, avg. loss 0.01 total examples 32, speed 1593.39 words/sec, time elapsed 90.74 sec\n",
    "I0308 14:54:52.695822 140735498576768 run.py:373] epoch 10, iter 39, avg. loss 0.01 total examples 32, speed 1593.36 words/sec, time elapsed 90.74 sec\n",
    "Loss is 0.009088872000575066\n",
    "epoch 10, iter 40, avg. loss 0.01 total examples 4, speed 1407.45 words/sec, time elapsed 91.10 sec\n",
    "I0308 14:54:53.059941 140735498576768 run.py:373] epoch 10, iter 40, avg. loss 0.01 total examples 4, speed 1407.30 words/sec, time elapsed 91.10 sec\n",
    "VALIDATION: 40 | Precision 1.0, recall 1.0, f1 1.0, accuracy: 1.0\n",
    "I0308 14:54:53.269808 140735498576768 run.py:389] VALIDATION: 40 | Precision 1.0, recall 1.0, f1 1.0, accuracy: 1.0\n",
    "hit patience 3\n",
    "I0308 14:54:53.270155 140735498576768 run.py:405] hit patience 3\n",
    "Loss is 0.009683635085821152\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "[WITH ADDITIONAL DROPOUT AND LINEAR LAYER]\n",
    "\n",
    "python run.py train --train-src=../data_toy/train.data --dev-src=../data_toy/test.data --vocab=vocab.toy.json --glove-path=../embeddings/glove.6B.100d.txt --target-length=128 --valid-niter=10 --model=reformer --transformer-depth=1 --transformer-heads=2 --log-every=1 --lr=0.0003\n",
    "\n",
    "epoch 10, iter 38, avg. loss 0.00 total examples 32, speed 1652.69 words/sec, time elapsed 80.66 sec\n",
    "I0308 14:57:13.627470 140735498576768 run.py:373] epoch 10, iter 38, avg. loss 0.00 total examples 32, speed 1652.66 words/sec, time elapsed 80.66 sec\n",
    "Loss is 0.002790470141917467\n",
    "epoch 10, iter 39, avg. loss 0.00 total examples 32, speed 1619.90 words/sec, time elapsed 83.19 sec\n",
    "I0308 14:57:16.156323 140735498576768 run.py:373] epoch 10, iter 39, avg. loss 0.00 total examples 32, speed 1619.87 words/sec, time elapsed 83.19 sec\n",
    "Loss is 0.004485769663006067\n",
    "epoch 10, iter 40, avg. loss 0.00 total examples 4, speed 1520.57 words/sec, time elapsed 83.52 sec\n",
    "I0308 14:57:16.493364 140735498576768 run.py:373] epoch 10, iter 40, avg. loss 0.00 total examples 4, speed 1520.24 words/sec, time elapsed 83.52 sec\n",
    "VALIDATION: 40 | Precision 1.0, recall 1.0, f1 1.0, accuracy: 1.0\n",
    "\n",
    "\n",
    "INSIGHTS\n",
    "- CLS token does not work well with the reformer model, likely because CLS token cannot attend with any neighboring tokens\n",
    "- Mask out any padding tokens and take average \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Test \n",
    "???\n",
    "\n",
    "### TINY\n",
    "#### Train \n",
    "python run.py train --train-src=../data_v2/train.tiny.data --dev-src=../data_v2/dev.tiny.data --vocab=vocab.json --target-length=256 --batch-size=4 --valid-niter=100 --model=reformer --transformer-depth=1 --transformer-heads=8\n",
    "\n",
    "Loss is 0.6570224165916443\n",
    "epoch 5, iter 20, avg. loss 0.66 total examples 4, speed 1181.07 words/sec, time elapsed 68.42 sec\n",
    "I0308 12:21:23.703120 140735498576768 run.py:373] epoch 5, iter 20, avg. loss 0.66 total examples 4, speed 1180.95 words/sec, time elapsed 68.42 sec\n",
    "VALIDATION: 20 | Precision 0.8, recall 1.0, f1 0.8666666666666666, accuracy: 0.8\n",
    "I0308 12:21:23.998986 140735498576768 run.py:389] VALIDATION: 20 | Precision 0.8, recall 1.0, f1 0.8666666666666666, accuracy: 0.8\n",
    "save currently the best model to [model.bin]\n",
    "I0308 12:21:23.999207 140735498576768 run.py:397] save currently the best model to [model.bin]\n",
    "save model parameters to [model.bin]\n",
    "\n",
    "#### Test \n",
    "???\n",
    "\n",
    "\n",
    "### FULL\n",
    "#### Train \n",
    "nohup python -u run.py train --train-src=../data_v2/train.data --dev-src=../data_v2/dev.data --vocab=vocab.json --target-length=1024 --batch-size=10 --valid-niter=1000 --model=reformer --save-to=model.reformer.bin --cuda --transformer-depth=1 --transformer-heads=8 &\n",
    "\n",
    "epoch 6, iter 18960, avg. loss 18.03 total examples 100, speed 35046.65 words/sec, time elapsed 6976.79 sec\n",
    "epoch 6, iter 18970, avg. loss 18.27 total examples 100, speed 35104.72 words/sec, time elapsed 6979.70 sec\n",
    "epoch 6, iter 18980, avg. loss 16.90 total examples 100, speed 35075.45 words/sec, time elapsed 6982.62 sec\n",
    "epoch 6, iter 18990, avg. loss 16.97 total examples 100, speed 35099.66 words/sec, time elapsed 6985.54 sec\n",
    "epoch 6, iter 19000, avg. loss 17.01 total examples 100, speed 35112.59 words/sec, time elapsed 6988.46 sec\n",
    "VALIDATION: 19000 | Precision 0.0, recall 0.0, f1 0.0, accuracy: 0.8807943469785273\n",
    "hit patience 3\n",
    "\n",
    "---\n",
    "\n",
    "[Start using target from tail]\n",
    "[Start using two hidden layers after reformer plus RELU]\n",
    "[nohup.out.reformer.2]\n",
    "\n",
    "nohup python -u run.py train --train-src=../data_v2/train.data --dev-src=../data_v2/test.data --vocab=vocab.json --target-length=1024 --valid-niter=1000 --model=reformer --transformer-depth=6 --transformer-heads=4 --log-every=1 --lr=0.0003 --threshold=0.1 --cuda --save-to=model.reformer.2.bin &\n",
    "\n",
    "epoch 9, iter 8998, avg. loss 0.19 total examples 32, speed 9597.57 words/sec, time elapsed 20333.80 sec\n",
    "03/09/2020 06:47:44 - INFO - drcoding -   epoch 9, iter 8998, avg. loss 0.19 total examples 32, speed 9597.43 words/sec, time elapsed 20333.80 sec\n",
    "Loss is 0.23645462095737457\n",
    "epoch 9, iter 8999, avg. loss 0.24 total examples 32, speed 19004.76 words/sec, time elapsed 20335.53 sec\n",
    "03/09/2020 06:47:45 - INFO - drcoding -   epoch 9, iter 8999, avg. loss 0.24 total examples 32, speed 19004.16 words/sec, time elapsed 20335.53 sec\n",
    "Loss is 0.1850530207157135\n",
    "epoch 9, iter 9000, avg. loss 0.19 total examples 32, speed 28245.82 words/sec, time elapsed 20336.69 sec\n",
    "03/09/2020 06:47:47 - INFO - drcoding -   epoch 9, iter 9000, avg. loss 0.19 total examples 32, speed 28244.62 words/sec, time elapsed 20336.69 sec\n",
    "VALIDATION: 9000 | Precision 0.7060970854859162, recall 0.3013251720220133, f1 0.3744705738270602, accuracy: 0.8939038892679114\n",
    "03/09/2020 06:57:39 - INFO - drcoding -   VALIDATION: 9000 | Precision 0.7060970854859162, recall 0.3013251720220133, f1 0.3744705738270602, accuracy: 0.8939038892679114\n",
    "save currently the best model to [model.reformer.2.bin]\n",
    "03/09/2020 06:57:39 - INFO - drcoding -   save currently the best model to [model.reformer.2.bin]\n",
    "save model parameters to [model.reformer.2.bin]\n",
    "\n",
    "---\n",
    "\n",
    "nohup python -u run.py train --train-src=../data_v2/train.data --dev-src=../data_v2/test.data --vocab=vocab.json --target-length=1024 --valid-niter=1000 --model=reformer --transformer-depth=6 --transformer-heads=4 --log-every=1 --lr=0.0003 --threshold=0.1 --save-to=model.reformer.metadata.bin --icd-desc-file=../data_v2/icd-desc.processed.txt --cuda &\n",
    "\n",
    "[nohup.out.reformer.3] <- accidentally ran with the same parameters as run 2\n",
    "\n",
    "VALIDATION: 15000 | Precision 0.7089888553140349, recall 0.3011542832514704, f1 0.37537666781318096, accuracy: 0.8938804951749192\n",
    "03/10/2020 05:30:42 - INFO - drcoding -   VALIDATION: 15000 | Precision 0.7089888553140349, recall 0.3011542832514704, f1 0.37537666781318096, accuracy: 0.8938804951749192\n",
    "save currently the best model to [model.reformer.metadata.bin]\n",
    "03/10/2020 05:30:42 - INFO - drcoding -   save currently the best model to [model.reformer.metadata.bin]\n",
    "save model parameters to [model.reformer.metadata.bin]\n",
    "\n",
    "^ Even though it says `model.reformer.metadata.bin`, it actually is a plain reformer model\n",
    "\n",
    "---\n",
    "\n",
    "nohup python3.5 -u run.py train --train-src=../data_v2/train.data --dev-src=../data_v2/test.data --vocab=vocab.json --target-length=2048 --batch-size=16 --valid-niter=1000 --model=reformer --transformer-depth=6 --transformer-heads=4 --log-every=1 --lr=0.0003 --top-k=5 --save-to=model.reformer.4.bin --icd-desc-file=../data_v2/icd-desc.processed.txt --cuda &\n",
    "\n",
    "[cs224n-2]\n",
    "[nohup.out.reformer.4]\n",
    "\n",
    "VALIDATION: 19000 | Precision 0.5489618871235101, recall 0.5190344824497314, f1 0.4900068225204126, accuracy: 0.888367287259929\n",
    "I0313 15:56:15.957444 140416226387712 run.py:439] VALIDATION: 19000 | Precision 0.5489618871235101, recall 0.5190344824497314, f1 0.4900068225204126, accuracy: 0.88836728725\n",
    "9929\n",
    "save currently the best model to [model.reformer.4.bin]\n",
    "I0313 15:56:15.957704 140416226387712 run.py:447] save currently the best model to [model.reformer.4.bin]\n",
    "save model parameters to [model.reformer.4.bin]\n",
    "\n",
    "---\n",
    "\n",
    "nohup python3.5 -u run.py train --train-src=../data_v2/train.data --dev-src=../data_v2/test.data --vocab=vocab.json --target-length=512 --batch-size=32 --valid-niter=1000 --model=reformer --transformer-depth=6 --transformer-heads=4 --log-every=1 --lr=0.0003 --top-k=5 --save-to=model.reformer.5.bin --icd-desc-file=../data_v2/icd-desc.processed.txt --cuda &\n",
    "\n",
    "[nohup.out.reformer.5]\n",
    "\n",
    "VALIDATION: 11000 | Precision 0.5077103031484554, recall 0.483944007627245, f1 0.45438570627180563, accuracy: 0.8801169704649252\n",
    "I0313 22:15:55.237652 139900908005120 run.py:439] VALIDATION: 11000 | Precision 0.5077103031484554, recall 0.483944007627245, f1 0.45438570627180563, accuracy: 0.8801169704649252\n",
    "save currently the best model to [model.reformer.5.bin]\n",
    "I0313 22:15:55.237902 139900908005120 run.py:447] save currently the best model to [model.reformer.5.bin]\n",
    "save model parameters to [model.reformer.5.bin]\n",
    "\n",
    "---\n",
    "\n",
    "nohup python3.5 -u run.py train --train-src=../data_v2/train.data --dev-src=../data_v2/dev.1000.data --vocab=vocab.json --target-length=256 --batch-size=32 --valid-niter=1000 --model=reformer --transformer-depth=6 --transformer-heads=4 --log-every=1 --lr=0.0003 --top-k=5 --save-to=model.reformer.6.bin --icd-desc-file=../data_v2/icd-desc.processed.txt --cuda &\n",
    "\n",
    "[nohup.out.reformer.6]\n",
    "\n",
    "---\n",
    "\n",
    "nohup python -u run.py train --train-src=../data_v2/train.data --dev-src=../data_v2/dev.1000.data --vocab=vocab.json --target-length=512 --batch-size=32 --valid-niter=1000 --model=reformer --transformer-depth=6 --transformer-heads=4 --log-every=1 --lr=0.0003 --top-k=5 --save-to=model.reformer.7.bin --icd-desc-file=../data_v2/icd-desc.processed.txt --cuda &\n",
    "[nohup.out.reformer.7]\n",
    "\n",
    "\n",
    "\n",
    "#### Test \n",
    "\n",
    "[nohup.out.reformer.2]\n",
    "\n",
    "python -u run.py predict model.reformer.2.bin ../data_v2/test.data ../predictions/reformer.txt --vocab-test=vocab.json --target-length=1024 --batch-size=64 --model=reformer --transformer-depth=6 --transformer-heads=4 --log-every=1 --cuda --threshold=0.1 --icd-desc-file=../data_v2/icd-desc.processed.txt\n",
    "\n",
    "Precision 0.7060970854859162, recall 0.3013446670995063, f1 0.374493782252647, accuracy: 0.8939038892679114\n",
    "03/11/2020 04:43:31 - INFO - drcoding -   Precision 0.7060970854859162, recall 0.3013446670995063, f1 0.374493782252647, accuracy: 0.8939038892679114\n",
    "\n",
    "---\n",
    "\n",
    "python -u run.py predict model.reformer.2.bin ../data_v2/test.data ../predictions/reformer-top-5.txt --vocab-test=vocab.json --target-length=1024 --batch-size=64 --model=reformer --transformer-depth=6 --transformer-heads=4 --log-every=1 --cuda --top-k=5 --icd-desc-file=../data_v2/icd-desc.processed.txt\n",
    "\n",
    "Precision 0.5329759235793007, recall 0.5087002570700528, f1 0.47749388482168637, accuracy: 0.8851700945510894\n",
    "03/11/2020 04:54:31 - INFO - drcoding -   Precision 0.5329759235793007, recall 0.5087002570700528, f1 0.47749388482168637, accuracy: 0.8851700945510894\n",
    "\n",
    "---\n",
    "\n",
    "python -u run.py predict model.reformer.2.bin ../data_v2/test.data ../predictions/reformer-top-8.txt --vocab-test=vocab.json --target-length=1024 --batch-size=64 --model=reformer --transformer-depth=6 --transformer-heads=4 --log-every=1 --cuda --top-k=8 --icd-desc-file=../data_v2/icd-desc.processed.txt\n",
    "\n",
    "Precision 0.4302929135393313, recall 0.6284303123446902, f1 0.47220684177037514, accuracy: 0.8562686421678212\n",
    "03/12/2020 06:47:38 - INFO - drcoding -   Precision 0.4302929135393313, recall 0.6284303123446902, f1 0.47220684177037514, accuracy: 0.8562686421678212\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformer Metadata Commands\n",
    "\n",
    "- Metadata scheme is definitely faster if using plain attention, but slower than LSH (because of increased batch sizes)\n",
    "- \n",
    "\n",
    "\n",
    "### TOY\n",
    "#### Train \n",
    "python run.py train --train-src=../data_toy/train.data --dev-src=../data_toy/test.data --vocab=vocab.toy.json --target-length=128 --valid-niter=10 --model=reformer-metadata --transformer-depth=1 --transformer-heads=2 --log-every=1 --lr=0.0003 --icd-desc-file=../data_toy/icd-desc.processed.txt\n",
    "\n",
    "\n",
    "### TINY\n",
    "#### Train \n",
    "python run.py train --train-src=../data_v2/train.tiny.data --dev-src=../data_v2/dev.tiny.data --vocab=vocab.json --target-length=1024 --batch-size=4 --valid-niter=100 --model=reformer-metadata --transformer-depth=1 --transformer-heads=8 --icd-desc-file=../data_v2/icd-desc.processed.txt\n",
    "\n",
    "python -u run.py train --train-src=../data_v2/train.tiny.data --dev-src=../data_v2/dev.tiny.data --vocab=vocab.json --target-length=512 --valid-niter=1000 --model=reformer-metadata --transformer-depth=6 --transformer-heads=4 --log-every=1 --lr=0.0003 --top-k=5 --save-to=model.reformer.metadata.5.bin --icd-desc-file=../data_v2/icd-desc.processed.txt --cuda\n",
    "\n",
    "\n",
    "### FULL\n",
    "#### Train \n",
    "nohup python -u run.py train --train-src=../data_v2/train.data --dev-src=../data_v2/test.data --vocab=vocab.json --target-length=1024 --valid-niter=1000 --model=reformer-metadata --transformer-depth=6 --transformer-heads=4 --log-every=1 --lr=0.0003 --threshold=0.1 --save-to=model.reformer.metadata.bin --icd-desc-file=../data_v2/icd-desc.processed.txt --cuda &\n",
    "\n",
    "[nohup.out.reformer.metadata]\n",
    "\n",
    "Notes:\n",
    "- This version takes the metadata IDs as the 'V' and 'K'\n",
    "- Metadata is repeated to match the sequence length (no padding)\n",
    "- Tail sequence\n",
    "- This version sucks because the model will just overfit on the ICD codes\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "nohup python -u run.py train --train-src=../data_v2/train.data --dev-src=../data_v2/test.data --vocab=vocab.json --target-length=1024 --valid-niter=1000 --model=reformer-metadata --transformer-depth=6 --transformer-heads=4 --log-every=1 --lr=0.0003 --threshold=0.1 --save-to=model.reformer.metadata.2.bin --icd-desc-file=../data_v2/icd-desc.processed.txt --cuda &\n",
    "\n",
    "Notes:\n",
    "- This version takes the metadata IDs as the 'K'\n",
    "- Metadata is repeated to match the sequence length (no padding)\n",
    "- Tail sequence\n",
    "\n",
    "[prematurely terminated due to VM shutdown]\n",
    "\n",
    "---\n",
    "\n",
    "nohup python -u run.py train --train-src=../data_v2/train.data --dev-src=../data_v2/test.data --vocab=vocab.json --target-length=1024 --valid-niter=1000 --model=reformer-metadata --transformer-depth=6 --transformer-heads=4 --log-every=1 --lr=0.0003 --threshold=0.1 --save-to=model.reformer.metadata.3.bin --icd-desc-file=../data_v2/icd-desc.processed.txt --cuda &\n",
    "\n",
    "[nohup.out.reformer.metadata.3]\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "nohup python -u run.py train --train-src=../data_v2/train.data --dev-src=../data_v2/test.data --vocab=vocab.json --target-length=1024 --valid-niter=1000 --model=reformer-metadata --transformer-depth=6 --transformer-heads=4 --log-every=1 --lr=0.0003 --threshold=0.1 --save-to=model.reformer.metadata.4.bin --icd-desc-file=../data_v2/icd-desc.processed.txt --cuda &\n",
    "\n",
    "Notes:\n",
    "- This version takes the metadata IDs as the 'K' and 'Q'\n",
    "- Metadata is repeated to match the sequence length (no padding)\n",
    "- Tail sequence\n",
    "\n",
    "TBD\n",
    "\n",
    "---\n",
    "\n",
    "nohup python -u run.py train --train-src=../data_v2/train.data --dev-src=../data_v2/test.data --vocab=vocab.json --target-length=512 --valid-niter=1000 --model=reformer-metadata --transformer-depth=6 --transformer-heads=4 --log-every=1 --lr=0.0003 --top-k=5 --save-to=model.reformer.metadata.5.bin --icd-desc-file=../data_v2/icd-desc.processed.txt --cuda &\n",
    "\n",
    "[nohup.out.reformer.metadata.5]\n",
    "\n",
    "---\n",
    "\n",
    "nohup python -u run.py train --train-src=../data_v2/train.data --dev-src=../data_v2/test.data --vocab=vocab.json --target-length=2048 --valid-niter=1000 --model=reformer-metadata --transformer-depth=6 --transformer-heads=4 --log-every=1 --lr=0.0003 --top-k=5 --save-to=model.reformer.metadata.5.bin --icd-desc-file=../data_v2/icd-desc.processed.txt --batch-size=8 --cuda &\n",
    "\n",
    "[nohup.out.reformer.metadata.6]\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "nohup python3.5 -u run.py train --train-src=../data_v2/train.data --dev-src=../data_v2/dev.1000.data --vocab=vocab.json --target-length=512 --batch-size=32 --valid-niter=1000 --model=reformer-metadata --transformer-depth=6 --transformer-heads=4 --log-every=10 --lr=0.0003 --top-k=5 --save-to=model.reformer.metadata.6.bin --icd-desc-file=../data_v2/icd-desc.processed.txt --cuda &\n",
    "[nohup.out.reformer.metadata.7]\n",
    "- factor of 2\n",
    "\n",
    "--\n",
    "\n",
    "nohup python3.5 -u run.py train --train-src=../data_v2/train.data --dev-src=../data_v2/dev.1000.data --vocab=vocab.json --target-length=512 --batch-size=32 --valid-niter=1000 --model=reformer-metadata --transformer-depth=6 --transformer-heads=4 --log-every=10 --lr=0.0003 --top-k=5 --save-to=model.reformer.metadata.8.bin --icd-desc-file=../data_v2/icd-desc.processed.txt --cuda &\n",
    "[nohup.out.reformer.metadata.8]\n",
    "- factor of 5\n",
    "\n",
    "--\n",
    "\n",
    "nohup python3.5 -u run.py train --train-src=../data_v2/train.data --dev-src=../data_v2/dev.1000.data --vocab=vocab.json --target-length=512 --batch-size=32 --valid-niter=1000 --model=reformer-metadata --transformer-depth=6 --transformer-heads=4 --log-every=10 --lr=0.0003 --top-k=5 --save-to=model.reformer.metadata.9.bin --icd-desc-file=../data_v2/icd-desc.processed.txt --cuda &\n",
    "[nohup.out.reformer.metadata.9]\n",
    "- uses keyword augmentation as a separate layer after the Reformer output\n",
    "- \n",
    "\n",
    "\n",
    "#### Test\n",
    "\n",
    "[nohup.out.reformer.metadata.3]\n",
    "- THRESHOLD = 0.1\n",
    "python -u run.py predict model.reformer.metadata.3.bin ../data_v2/test.data ../predictions/reformer-metadata.txt --vocab-test=vocab.json --target-length=1024 --batch-size=64 --model=reformer-metadata --transformer-depth=6 --transformer-heads=4 --log-every=1 --cuda --threshold=0.1 --icd-desc-file=../data_v2/icd-desc.processed.txt\n",
    "\n",
    "Precision 0.7210595574617442, recall 0.29552728741758655, f1 0.37249723334494744, accuracy: 0.893716736523978\n",
    "03/11/2020 04:42:54 - INFO - drcoding -   Precision 0.7210595574617442, recall 0.29552728741758655, f1 0.37249723334494744, accuracy: 0.893716736523978\n",
    "\n",
    "---\n",
    "\n",
    "[nohup.out.reformer.metadata.3]\n",
    "- TOP 5 \n",
    "python -u run.py predict model.reformer.metadata.3.bin ../data_v2/test.data ../predictions/reformer-metadata-top-5.txt --vocab-test=vocab.json --target-length=1024 --batch-size=64 --model=reformer-metadata --transformer-depth=6 --transformer-heads=4 --log-every=1 --cuda --top-k=5 --icd-desc-file=../data_v2/icd-desc.processed.txt\n",
    "\n",
    "Precision 0.5330539038892727, recall 0.5095594315829758, f1 0.4779807523749932, accuracy: 0.8851856906130828\n",
    "03/11/2020 04:48:50 - INFO - drcoding -   Precision 0.5330539038892727, recall 0.5095594315829758, f1 0.4779807523749932, accuracy: 0.8851856906130828\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drcoding",
   "language": "python",
   "name": "drcoding"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
